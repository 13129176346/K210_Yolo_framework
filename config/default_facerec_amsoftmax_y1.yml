BATCH_SIZE_REF: &BATCH_SIZE_REF 64
WIDTH_REF: &WIDTH_REF 112

mode: train
model:
  name: feacrec
  helper: FcaeRecHelper
  helper_kwarg:
    image_ann: data/ms1m_img_ann.npy
    in_hw:
      - *WIDTH_REF
      - *WIDTH_REF
    embedding_size: 128
    val_dataset: lfw,cfp_fp,agedb_30
    use_softmax: true
  network: FMobileFaceNet_eager
  network_kwarg:
    input_shape:
      - *WIDTH_REF
      - *WIDTH_REF
      - 3
    class_num: 85742
    embedding_size: 128
    blocks: [1, 4, 6, 2]
    loss: amsoftmax
    act_type: leakyrelu
    bn_mom: 0.9
train:
  graph_optimizer: true
  graph_optimizer_kwarg: 
    layout_optimizer: true
    remapping: true
    shape_optimization: true
    loop_optimization: true
    constant_folding: true
    function_optimization: true
    scoped_allocator_optimization: true
  augmenter: true
  batch_size: *BATCH_SIZE_REF
  pre_ckpt: null
  rand_seed: 10101
  epochs: 100
  train_epoch_step: 2000
  vali_epoch_step: null
  steps_per_run: 30
  log_dir: log
  sub_log_dir: default_facerec_amsoftmax_y1_exp
  mixed_precision:
    enable: false 
    dtype: float32
  trainloop: FaceSoftmaxTrainingLoop
  trainloop_kwarg: 
    hparams:
      loss:
        name: Sparse_AmsoftmaxLoss
        kwarg: 
          batch_size: *BATCH_SIZE_REF
          scale: 30
          margin: 0.35
  variablecheckpoint_kwarg: 
    variable_dict:
      model: train_model
      optimizer: optimizer
    monitor: train/acc
    mode: max
  optimizer: Adam
  optimizer_kwarg:
    learning_rate: 0.001
    beta_1: 0.9
    beta_2: 0.999
    amsgrad: false
  callbacks:
    # - name: EarlyStopping
    #   kwarg:
    #     monitor: train/acc
    #     min_delta: 0
    #     patience: 10
    #     verbose: 0
    #     mode: max
    #     baseline: null
    #     restore_best_weights: false
    - name: StepLR
      kwarg:
        steps: [ 10  ,  80    ,  120   ]
        rates: [0.0001, 0.0008,  0.0004]
