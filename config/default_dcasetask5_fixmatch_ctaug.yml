UPDATE_AUGMENTER_STATE: &UPDATE_AUGMENTER_STATE true
MIXED_PRECISION_DTYPE: &MIXED_PRECISION_DTYPE float32

mode: train
model:
  name: fixmatchaudio
  helper: DCASETask5FixMatchSSLHelper
  helper_kwarg:
    data_ann: data/dcasetask5_ann_list.npy
    in_hw:
      - 40
      - 501
    nclasses: 9
    unlabel_dataset_ratio: 3 # Unlabeled batch size ratio.
    mixed_precision_dtype: *MIXED_PRECISION_DTYPE
    augment_kwargs:
      name: ctaugment
      kwarg:
        num_layers: 2
        confidence_threshold: 0.9
        decay: 0.99
        epsilon: 0.001
        prob_to_apply: 0.5
        num_levels: 10
  network: dcasetask5basemodel
  network_kwarg:
    input_shape:
      - 40
      - 501
      - 1
    nclasses: 9
    softmax: false
    weight_decay: 0.0003
    bn_decay: 0.9
    bn_epsilon: 0.00001
train:
  graph_optimizer: true
  graph_optimizer_kwarg:
    layout_optimizer: true
    remapping: true
    shape_optimization: true
    loop_optimization: true
    constant_folding: true
    function_optimization: true
    scoped_allocator_optimization: true
  distributionstrategy_kwarg:
    tpu: null
    strategy: Mirrored
  augmenter: true
  batch_size: 16
  pre_ckpt: null
  rand_seed: 10101
  epochs: 100
  train_epoch_step: null
  vali_epoch_step: null
  steps_per_run: 30
  log_dir: log
  sub_log_dir: default_dcasetask5_fixmatch_ctaug_exp
  mixed_precision:
    enable: false
    dtype: *MIXED_PRECISION_DTYPE
  trainloop: Task5FixMatchSslLoop
  trainloop_kwarg:
    hparams:
      update_augmenter_state: *UPDATE_AUGMENTER_STATE
      fixmatch:
        wu: 1.0 # Pseudo label loss weight
        confidence: 0.7 # Confidence threshold.
      ema:
        enable: true
        decay: 0.999
  variablecheckpoint_kwarg:
    variable_dict:
      train_model: train_model
      val_model: val_model
      ema_model: loop.ema.model
      optimizer: optimizer
    monitor: val/acc
    mode: max
  # optimizer: Adam
  # optimizer_kwarg:
  #   learning_rate: 0.001
  #   beta_1: 0.9
  #   beta_2: 0.999
  #   amsgrad: false
  optimizer: SGD
  optimizer_kwarg:
    learning_rate: 0.001
    momentum: 0.9
    nesterov: True
  callbacks:
    - name: EarlyStopping
      kwarg:
        monitor: val/acc
        min_delta: 0
        patience: 50
        verbose: 0
        mode: max
        baseline: null
        restore_best_weights: false
    - name: AugmenterStateSync
      kwarg:
        update_augmenter_state: *UPDATE_AUGMENTER_STATE
    - name: ScheduleLR
      kwarg:
        base_lr: 0.001
        use_warmup: true
        warmup_epochs: 20
        decay_rate: 0.7
        decay_epochs: 5
        outside_optimizer: optimizer
